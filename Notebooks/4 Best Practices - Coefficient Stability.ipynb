{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd0a331",
   "metadata": {},
   "source": [
    "# Best Practices in Coefficient Stability\n",
    "Julian Hsu\n",
    "1 March 2023\n",
    "\n",
    "The purpose of this script is to demonstrate the idea of Coefficient Stability following [Oster 2013](https://www.nber.org/system/files/working_papers/w19054/w19054.pdf) which is in the same spirit as [Imbens 2003](https://www.aeaweb.org/articles?id=10.1257/000282803321946921).\n",
    "\n",
    "At a high level, this coefficient stability metric answers the question of how much bias would unobserved variables potential control for so that it could increase $R^2$ a certain level and change the treatment estimate by a certain level. In other words, given what we can predict and estimate with what we can observe, how dramatic must the omitted variable be? This requires user input of how much it could matter.\n",
    "\n",
    "*Oster 2013* combines the omitted variable bias formula and $R^2$ formulas to calculate a statistic $\\delta^*$:\n",
    "\n",
    "$$\\delta^* =$$\n",
    "$$ \\dfrac{(\\tilde{\\beta} - \\hat{\\beta}) (\\tilde{R} - \\dot{R}) \\hat{\\sigma}^2_y \\hat{\\tau}_x + (\\tilde{\\beta} - \\hat{\\beta})\\hat{\\sigma}^2_x \\hat{\\tau}_x (\\dot{\\beta} - \\tilde{\\beta})^2 + 2A   }{(R_{max} - \\tilde{R}) \\hat{\\sigma}^2_y (\\dot{\\beta} - \\tilde{\\beta})\\hat{\\sigma}^2_x + (\\tilde{\\beta} - \\hat{\\beta})(R_{max} - \\tilde{R})\\hat{\\sigma}^2_y (\\hat{\\sigma}^2_x - \\hat{\\tau}_x) + A } $$\n",
    "\n",
    "where $A=(\\tilde{\\beta}-\\hat{\\beta})^2 (\\hat{\\tau}_x (\\dot{\\beta}-\\tilde{\\beta})\\hat{\\sigma}^2_x ) + (\\tilde{\\beta}-\\hat{\\beta})^3(\\hat{\\tau}_x \\hat{\\sigma}^2_x - \\hat{\\tau}^2_x) $, \n",
    "\n",
    "and $\\delta$ is the proportion of selection of observed and unobserved features. $\\delta \\frac{\\sigma_{1x}}{\\sigma^2_1} = \\frac{\\sigma_{2x}}{\\sigma^2_2}$, and $\\frac{\\sigma_{1x}}{\\sigma^2_1}$ is the coefficient of the variable of interest with observe and $\\frac{\\sigma_{2x}}{\\sigma^2_2}$ is the same but with unobserved. **This means that $\\delta$ should be interpreted as the amount of selection on unobserved needed such that the estimate we have $\\hat{\\beta}$ becomes $\\tilde{\\beta}$.**\n",
    "\n",
    "The inputs for $\\delta^*$ are:\n",
    "- $\\dot{\\beta}$ is from regression $Y = \\dot{\\beta} X_{interest} $\n",
    "- $\\tilde{\\beta}$ is from regression $Y = \\dot{\\beta} X_{interest} + f(X_{observed})$\n",
    "- $\\hat{\\beta}$ [**user determined**] is from regression $Y = \\dot{\\beta} X_{interest} + f(X_{observed}, X_{unobserved})$\n",
    "- $\\hat{\\sigma}^2_Y$ is the covariance of $Y$\n",
    "- $\\hat{\\sigma}^2_X$ is the covariance of $X_{interest} $\n",
    "- $\\hat{\\tau}_X$ is the variance of $(X_{interest} - \\mathbb{E}[X_{interest} | X_{observed}] )$\n",
    "- $R_{max}$ [**user determined**] is the maximum explain variance from including observed and unobserved.\n",
    "- $\\tilde{R}$ is the explained variation of observed\n",
    "- $\\dot{R}$ is the explained variation when you only have the feature of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a806c0",
   "metadata": {},
   "source": [
    "The remainder of this notebook is going to show how this works in practice, by generating simulated datasets with different DGPs and see whether estimates of $\\delta^*$ can inform whether we have the correct causal estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3aa27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import os as os\n",
    "home = os.getcwd()\n",
    "import stnomics as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0362b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp(N=1000, K=10, Knotimportant=2, ate=5):\n",
    "    \n",
    "    assert Knotimportant < K,'Knotimportant must be less than K so that a subset of the features are not relevant to the problem.'\n",
    "    df = pd.DataFrame()\n",
    "    ## Create the dataset\n",
    "    for k in range(K):\n",
    "        df['x{0}'.format(k)] = np.random.uniform(-1,1,N)\n",
    "    ## Create the propensity score and baseline outcome conditional on the important features\n",
    "    ## Allow for more than just a linear relationship, but also squared and cosine ones.\n",
    "    \n",
    "    df['latent_prop'] = np.dot(df[['x{0}'.format(e) for e in range(K) if e < K-Knotimportant]],\n",
    "                               np.random.uniform(-2,2, K-Knotimportant)) + np.random.normal(0,1,N)\n",
    "    df['latent_prop'] += np.dot(df[['x{0}'.format(e) for e in range(K) if e < K-Knotimportant]].pow(2),\n",
    "                               np.random.uniform(-2,2, K-Knotimportant))\n",
    "    df['latent_prop'] += np.dot( 3*np.cos(df[['x{0}'.format(e) for e in range(K) if e < K-Knotimportant]]),\n",
    "                               np.random.uniform(-2,2, K-Knotimportant))\n",
    "    \n",
    "    df['Y'] = np.dot(df[['x{0}'.format(e) for e in range(K) if e < K-Knotimportant]],\n",
    "                               np.random.uniform(-2,2, K-Knotimportant)) + np.random.normal(0,1,N)\n",
    "    df['Y'] += np.dot(df[['x{0}'.format(e) for e in range(K) if e < K-Knotimportant]].pow(2),\n",
    "                               np.random.uniform(-2,2, K-Knotimportant))\n",
    "    df['Y'] += np.dot( np.cos(3*df[['x{0}'.format(e) for e in range(K) if e < K-Knotimportant]]),\n",
    "                               np.random.uniform(-2,2, K-Knotimportant))\n",
    "        \n",
    "    df['latent_prop'] = np.exp(df['latent_prop']) / (1+np.exp(df['latent_prop']) )\n",
    "    df['W'] = (df['latent_prop'] > np.percentile(df['latent_prop'],0.50) ).astype(float)\n",
    "    df['Y'] += df['W']*ate\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "731e1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "allK=15\n",
    "notimportantK=3\n",
    "df = dgp(N=1000, K=allK, Knotimportant=notimportantK)\n",
    "df.describe().T\n",
    "feature_list = [e for e in df.columns if 'x' in e]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef8094",
   "metadata": {},
   "source": [
    "We now want to control for different variables, each time reporting the treatment effect. Controlling for one more variable at a time, we want to know what the order is. We will order features in decreasing order of their importance to predicting Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "763eb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fselect = st.diagnostics.selection.hdm_selection(df, feature_list, 'Y','W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b66c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "fselect_df = fselect[-1]\n",
    "fselect_df['coef_abs'] = fselect_df['coef'].abs()\n",
    "# print(fselect_df.loc[fselect_df['type']=='outcome'])\n",
    "outcome_feature_list = fselect_df.loc[fselect_df['type']=='outcome'].sort_values('coef_abs',ascending=False)['features'].to_list()\n",
    "treatment_feature_list = fselect_df.loc[fselect_df['type']=='treatment'].sort_values('coef_abs',ascending=False)['features'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c209e7",
   "metadata": {},
   "source": [
    "We now want to calculcate for a given set of control variables:\n",
    "1. R2; and\n",
    "2. $\\beta$ estimate of the treatment effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96f2f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def te_r2_output(te_model, te_model_dict_inputs,\n",
    "                y_model):\n",
    "    '''\n",
    "    This function outputs treatment estimates, standard, and the R-squared from any general model.\n",
    "    '''\n",
    "    \n",
    "    ## Call the treatment effect estimator\n",
    "    te = te_model(te_model_dict_inputs['data_est'],\n",
    "             te_model_dict_inputs['split_name'],\n",
    "             te_model_dict_inputs['feature_name'],\n",
    "             te_model_dict_inputs['outcome_name'],\n",
    "             te_model_dict_inputs['treatment_name'],\n",
    "             te_model_dict_inputs['ymodel'],\n",
    "             te_model_dict_inputs['tmodel'],\n",
    "             te_model_dict_inputs['n_data_splits'],\n",
    "             te_model_dict_inputs['aux_dictionary'])\n",
    "\n",
    "    yhat = st.predict_continuous(te_model_dict_inputs['data_est'],\n",
    "                             te_model_dict_inputs['split_name'],\n",
    "                             te_model_dict_inputs['n_data_splits'],\n",
    "                             te_model_dict_inputs['feature_name'],\n",
    "                             te_model_dict_inputs['outcome_name'],\n",
    "                              y_model\n",
    "                             )\n",
    "    rsquared = r2_score( te_model_dict_inputs['data_est'][te_model_dict_inputs['outcome_name']], yhat)\n",
    "    return te['ATE TE'], te['ATE SE'], rsquared\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ed6fe09",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12628\\4044683826.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0miterative_ml_rsquared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     d = st.diagnostics.coefstab.delta_ml(df, 'W', 'Y', \n\u001b[0m\u001b[0;32m     32\u001b[0m                    \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                   \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdml_plm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_model_dict_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stnomics.py\u001b[0m in \u001b[0;36mdelta_ml\u001b[1;34m(df, W, y, beta_hat, R_max, te_model, te_model_dict_inputs, y_model)\u001b[0m\n\u001b[0;32m    496\u001b[0m                 model_on_train = te_model_dict_inputs['tmodel'].fit(sm.add_constant(train[te_model_dict_inputs['feature_name']]), \n\u001b[0;32m    497\u001b[0m                                                                     train[W] )\n\u001b[1;32m--> 498\u001b[1;33m                 \u001b[0mmodel_predict_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_on_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mte_model_dict_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m                 \u001b[0mW_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_predict_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "ymodel = RandomForestRegressor(n_jobs=-1, n_estimators=200)\n",
    "tmodel = RandomForestClassifier(n_jobs=-1, n_estimators=200)\n",
    "te_model_dict_inputs = {\n",
    "    'data_est': df,\n",
    "    'split_name': 'splits',\n",
    "    'n_data_splits': 5,\n",
    "    'feature_name': feature_list,\n",
    "    'outcome_name': 'Y',\n",
    "    'treatment_name': 'W',\n",
    "    'ymodel':ymodel,\n",
    "    'tmodel':tmodel,\n",
    "    'aux_dictionary': {'lower':0.001, 'upper': 0.999}\n",
    "}\n",
    "\n",
    "iterative_ml_list = []\n",
    "iterative_ml_coeff = []\n",
    "iterative_ml_se = []\n",
    "iterative_ml_rsquared = []\n",
    "iterative_delta_ml = []\n",
    "\n",
    "for f in outcome_feature_list:    \n",
    "    ## ML models\n",
    "    iterative_ml_list.append(f)\n",
    "    te_model_dict_inputs['feature_name'] = iterative_ml_list\n",
    "    a = te_r2_output(st.ate.dml.dml_plm, te_model_dict_inputs,\n",
    "                    ymodel)\n",
    "    iterative_ml_coeff.append(a[0])\n",
    "    iterative_ml_se.append(a[1])\n",
    "    iterative_ml_rsquared.append(a[2])\n",
    "    \n",
    "    d = st.diagnostics.coefstab.delta_ml(df, 'W', 'Y', \n",
    "                   0, 1,\n",
    "                  st.ate.dml.dml_plm, te_model_dict_inputs,\n",
    "                ymodel)\n",
    "    iterative_delta_ml.append(d)    \n",
    "\n",
    "    \n",
    "iterative_ml_coeff = np.array(iterative_ml_coeff)\n",
    "iterative_ml_se = np.array(iterative_ml_se)\n",
    "iterative_ml_rsquared = np.array(iterative_ml_rsquared)\n",
    "iterative_delta_ml = np.array(iterative_delta_ml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0ab14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(14,2.5), sharex=True)\n",
    "x_index = np.arange(len(outcome_feature_list))\n",
    "ax[0].scatter(x_index, iterative_ml_coeff)\n",
    "ax[1].scatter(x_index, iterative_ml_rsquared)\n",
    "ax[2].scatter(x_index, iterative_delta_ml)\n",
    "ax[0].set_xlabel('Controlling for an Additional Feature')\n",
    "ax[0].set_ylabel('Coefficient Estimate')\n",
    "ax[1].set_ylabel('R-Squared')\n",
    "ax[2].set_ylabel('\\Delta*')\n",
    "ax[0].set_xticks(x_index,[outcome_feature_list[e]+', not imp.'*(e>=allK-notimportantK ) for e in range(len(outcome_feature_list))],\n",
    "                rotation=45)\n",
    "ax[1].set_xticks(x_index,[outcome_feature_list[e]+', not imp.'*(e>=allK-notimportantK ) for e in range(len(outcome_feature_list))],\n",
    "                rotation=45)\n",
    "ax[2].set_xticks(x_index,[outcome_feature_list[e]+', not imp.'*(e>=allK-notimportantK ) for e in range(len(outcome_feature_list))],\n",
    "                rotation=45)\n",
    "plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcde24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
