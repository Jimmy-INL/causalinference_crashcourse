{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7a5ff6",
   "metadata": {},
   "source": [
    "# Conformal Inference\n",
    "Julian Hsu\n",
    "5jun2022\n",
    "\n",
    "In this notebook we look at the idea of conformal inference from *Chernozhukov, Wuthrich, and Zhu* : https://arxiv.org/abs/1712.09089. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "print (os.getcwd() )\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import scipy\n",
    "import functools\n",
    "import sklearn as sk\n",
    "from sklearn import impute, pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd056f9e",
   "metadata": {},
   "source": [
    "## Permutations to get P-Values\n",
    "Implement the $iid$ permutation approach. Given a set of time period, go through different iterations in that time period. \n",
    "\n",
    "Our setup is for the number pre-treatment periods $T_0$ and the number of post-treatment periods $T_1$. $\\hat{u}_t$ is the difference between the estimated proxy outcome and realized outcome.\n",
    "$$S_q(\\hat{u}) = (T_1^{-1/2}  \\sum^T_{t = T_0 + 1}|\\hat{u}_t|^q )^{1/q}$$\n",
    "**Note that $S_q(\\hat{u})$ is calculated without any permutations or scrambling of the data.** In order to get p-values, we need to do multiple permutations, each with their own instance of $S_q(\\hat{u})$. Denote each of the permutations $\\pi \\in \\Pi$ as $S_q(\\hat{u}_{\\pi})$. The p-value is then:\n",
    "\n",
    "$$\\hat{p} = 1 - \\dfrac{1}{\\Pi} \\sum_{\\pi \\in \\Pi} 1\\{ S_q(\\hat{u}_{\\pi}) < S_q(\\hat{u})  \\} $$\n",
    "\n",
    "We can also calculate the confidence interval by a grid search process. Consider a lot of different treatment effects - for example if you are interested in treatment effects that are constant in all post-treatment periods, it is just one number. Each treatment effect is a \"null hypotheses\" you incorporate when training the model, so you can calculate $\\hat{p}$ by seeing whether it is different from that treatment effect \"null hypothesis.\" Then estimate the confidence interval of $1-\\alpha$ (or set, since we are doing a grid search) where those $\\hat{p}$ are greater than tha False Positive Rate $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b2671",
   "metadata": {},
   "source": [
    "Therefore, our implementation algorithm is:\n",
    "1. Don't scramble anything and train the model.\n",
    "    1. Estimate the treatment effect.\n",
    "    2. Estimate the residual and calculate $S_q({\\hat{u})$.\n",
    "    \n",
    "2. Pick one way of scrambling the time periods, $\\pi$;\n",
    "3. Estimate the model based on that scrambling, $\\pi$.  Calculate $S_q({\\hat{u}_\\pi)$.\n",
    "4. Calculate the p-value, $\\hat{p}$.\n",
    "5. If we are interested, calculate the confidence set by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ee985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
