{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780fba3b",
   "metadata": {},
   "source": [
    "# Conformal Inference\n",
    "Julian Hsu\n",
    "5jun2022\n",
    "\n",
    "In this notebook we look at the idea of conformal inference from *Chernozhukov, Wuthrich, and Zhu* : https://arxiv.org/abs/1712.09089. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5422af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hsujulia/Documents/GitHub/causalinference_crashcourse/Notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "print (os.getcwd() )\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import scipy\n",
    "import functools\n",
    "import sklearn as sk\n",
    "from sklearn import impute, pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d59bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to automatically give us a panel dataset\n",
    "def synth_panel(seed_number = 2123, \n",
    "                time_units = 10, \n",
    "                x_units = 100, \n",
    "                K_dim = 5,\n",
    "               treatment_type = 'both',\n",
    "               treatment_imp = 10,\n",
    "               treatment_het = False):\n",
    "    '''\n",
    "    seed_number  random_number_set\n",
    "    time_units   how many time periods?\n",
    "    x_units      how many units?\n",
    "    K_dim        how many covariates (assume uniform)\n",
    "    treatment_type    how is treatment determined? {'X','Y', 'both'}\n",
    "    treatment_imp     the level of treatment effect, constant for all post-treatment times\n",
    "    '''\n",
    "    ## Set the seed for random number\n",
    "    xfun = np.random.RandomState(seed_number)\n",
    "    \n",
    "    ## Construct covariates, time-invariant fixed effects, and first period outcome\n",
    "    X = xfun.uniform(low=0, high=1, size= (x_units, K_dim) )\n",
    "    X_coef = xfun.uniform(low=-2, high=2, size = K_dim)\n",
    "    x_list = ['x'+str(k) for k in range(K_dim)]\n",
    "    \n",
    "    \n",
    "    fe = xfun.normal(0,1, size=x_units)\n",
    "    \n",
    "    output_df = pd.DataFrame(data=X, columns=x_list)\n",
    "    output_df['fe'] = fe\n",
    "    output_df['Y'] = np.dot(X,X_coef) + fe + xfun.normal(0,1, size=x_units)\n",
    "    output_df['T'] = pd.to_datetime('2020/01/01')\n",
    "    output_df['unitid'] =  np.arange(x_units).astype(int)\n",
    "\n",
    "    decay = 0.60\n",
    "\n",
    "    for t in range(1,time_units):    \n",
    "        ## Create the panel by setting up an AR(1) type function.\n",
    "        prev_date = (pd.to_datetime('2020/01/01')+pd.offsets.DateOffset(months=t-1))\n",
    "        prev_data = decay*( output_df.loc[(output_df['T']==prev_date)][[r for r in output_df.columns if 'x' in r ]]     )\n",
    "        ## perturb it a little with AR(1)\n",
    "        prev_data += xfun.normal(0,1, size=(x_units,K_dim))\n",
    "        prev_data['Y'] = decay*np.dot(prev_data,X_coef) + fe + xfun.normal(0,1, size=x_units)\n",
    "        prev_data['T'] = pd.to_datetime('2020/01/01') + pd.offsets.DateOffset(months=t)\n",
    "        prev_data['unitid'] = np.arange(x_units).astype(int)\n",
    "        output_df = pd.concat([ output_df, prev_data])\n",
    "    output_df['unitid'] = 'unit' + output_df['unitid'].apply(str)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Decide the treatment effect based either the (i) covariates, (ii) pre-trend outcome, or (iii) both.\n",
    "    \n",
    "    Assign treatment to happen at the third to last time period\n",
    "    '''\n",
    "    if treatment_type=='X':\n",
    "        input_treatment = x_list[:]\n",
    "    elif treatment_type=='Y':\n",
    "        input_treatment = 'Y'\n",
    "    else:\n",
    "        input_treatment = x_list[:] + ['Y']\n",
    "        \n",
    "    output_df['treatment_latent'] = 0\n",
    "    treatment_date = (pd.to_datetime('2020/01/01')+pd.offsets.DateOffset(months=time_units-3))    \n",
    "    \n",
    "    latent_treatment = np.dot(output_df[input_treatment], \n",
    "                              xfun.uniform(-4,4, size=len(input_treatment))) +\\\n",
    "                             xfun.normal(0,1,size=len(output_df))\n",
    "    output_df['treatment_latent'] = np.exp(latent_treatment) / (1+ np.exp(latent_treatment))\n",
    "    \n",
    "    ## First decide treatment in the treatment time, and then make it permanent.    \n",
    "    treatment_latent_xsection = output_df.loc[(output_df['T'] == treatment_date)][['unitid','treatment_latent']]\n",
    "    treatment_decision = (treatment_latent_xsection['treatment_latent'] >\\\n",
    "                          treatment_latent_xsection['treatment_latent'].quantile(q=0.75))\n",
    "    treatment_ids = treatment_latent_xsection.loc[(treatment_decision==True)]['unitid']\n",
    "    \n",
    "    output_df['post'] = (output_df['T'] >= treatment_date).astype(float)\n",
    "    output_df['treatment_units'] = (output_df['unitid'].isin(treatment_ids)).astype(float)\n",
    "    \n",
    "    output_df['treatment'] = 0\n",
    "    output_df.loc[ (output_df['unitid'].isin(treatment_ids)) & (output_df['T'] >= treatment_date), 'treatment'  ] = 1\n",
    "\n",
    "    '''\n",
    "    Apply treatment effect. \n",
    "    if we allow treatment heterogeneity, assign treatment such that the ATE!=ATET\n",
    "    '''\n",
    "    if treatment_het==False:\n",
    "        output_df['treatment_GT']  = treatment_imp\n",
    "    else:\n",
    "        ## Calculate the proportion treated\n",
    "        treat_mean = output_df['treatment'].mean()\n",
    "        output_df['treatment_GT'] = treatment_imp/2\n",
    "        output_df.loc[(output_df['treatment']==1), 'treatment_GT'] = treatment_imp*2\n",
    "\n",
    "        \n",
    "    output_df.loc[( output_df['treatment']==1) , 'Y'] = output_df.loc[( output_df['treatment']==1)]['treatment_GT'] + output_df.loc[( output_df['treatment']==1)]['Y']\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a68e31",
   "metadata": {},
   "source": [
    "## Permutations to get P-Values\n",
    "Implement the $iid$ permutation approach. Given a set of time period, go through different iterations in that time period. \n",
    "\n",
    "Our setup is for the number pre-treatment periods $T_0$ and the number of post-treatment periods $T_1$. $\\hat{u}_t$ is the difference between the estimated proxy outcome and realized outcome.\n",
    "$$S_q(\\hat{u}) = (T_1^{-1/2}  \\sum^T_{t = T_0 + 1}|\\hat{u}_t|^q )^{1/q}$$\n",
    "**Note that $S_q(\\hat{u})$ is calculated without any permutations or scrambling of the data.** In order to get p-values, we need to do multiple permutations, each with their own instance of $S_q(\\hat{u})$. Denote each of the permutations $\\pi \\in \\Pi$ as $S_q(\\hat{u}_{\\pi})$. The p-value is then:\n",
    "\n",
    "$$\\hat{p} = 1 - \\dfrac{1}{\\Pi} \\sum_{\\pi \\in \\Pi} 1\\{ S_q(\\hat{u}_{\\pi}) < S_q(\\hat{u})  \\} $$\n",
    "\n",
    "We can also calculate the confidence interval by a grid search process. Consider a lot of different treatment effects - for example if you are interested in treatment effects that are constant in all post-treatment periods, it is just one number. Each treatment effect is a \"null hypotheses\" you incorporate when training the model, so you can calculate $\\hat{p}$ by seeing whether it is different from that treatment effect \"null hypothesis.\" Then estimate the confidence interval of $1-\\alpha$ (or set, since we are doing a grid search) where those $\\hat{p}$ are greater than tha False Positive Rate $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4028e4",
   "metadata": {},
   "source": [
    "Therefore, our implementation algorithm is:\n",
    "1. Don't scramble anything and train the model.\n",
    "    1. Estimate the treatment effect.\n",
    "    2. Estimate the residual and calculate $S_q({\\hat{u})$.\n",
    "    \n",
    "2. Pick one way of scrambling the time periods, $\\pi$;\n",
    "3. Estimate the model based on that scrambling, $\\pi$.  Calculate $S_q({\\hat{u}_\\pi)$.\n",
    "4. Calculate the p-value, $\\hat{p}$.\n",
    "5. If we are interested, calculate the confidence set by "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7b3c4",
   "metadata": {},
   "source": [
    "For simplicity sake, let's just use synthetic control following Abadie, Diamond, and Hainmueller (2010), which does an unpenalized synthetic control (SC) model. We can later on incorporate difference-in-difference (DiD) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed5d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abadie/Diamond/Hainmueller    \n",
    "from typing import List\n",
    "from operator import add\n",
    "from toolz import reduce, partial\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "## Define loss function\n",
    "def loss_w(W, X, y) -> float:\n",
    "    return np.sqrt(np.mean((y - X.dot(W))**2))\n",
    "\n",
    "def get_w(X, y):\n",
    "    ## Initialize at sample average with some noise\n",
    "    w_start = [1/X.shape[1]]*X.shape[1]+np.random.uniform(-0.005,0.005, X.shape[1])\n",
    "#     w_start = np.ones(X.shape[1])\n",
    "\n",
    "    weights = fmin_slsqp(partial(loss_w, X=X, y=y),\n",
    "                         np.array(w_start),\n",
    "                         f_eqcons=lambda x: np.sum(x) - 1,\n",
    "                         iter=50000, \n",
    "                         bounds=[(0.0, 1.0)]*len(w_start),\n",
    "                         disp=False)\n",
    "    return weights\n",
    "\n",
    "def syncontrol(X1=None,X0=None,\n",
    "               Y1=None,Y0=None,\n",
    "              scaledown = None):\n",
    "    '''\n",
    "    X1   control post-treatment\n",
    "    X0   control pre-treatment\n",
    "    Y1   treatment post-treatment\n",
    "    Y0   treatment pre-treatment\n",
    "    scaledown   whether to divide everything by a number, defaulted to none\n",
    "    '''\n",
    "    \n",
    "    if scaledown is None:\n",
    "        adh_weights = get_w(X0, Y0)\n",
    "    else:\n",
    "        adh_weights = get_w(X0/scaledown, Y0/scaledown)\n",
    "    ## Output the weights\n",
    "    \n",
    "    ## Output the counterfactual control outcome\n",
    "    Y_hatpre = np.dot(X0,adh_weights)\n",
    "    Y_hatpst = np.dot(X1,adh_weights)\n",
    "    \n",
    "    ## Output the average treatment effect on treated\n",
    "    atet = np.average(Y1 - Y_hatpst)\n",
    "    \n",
    "    return {'weights': adh_weights, 'control_pre': Y_hatpre,\n",
    "            'control_pst': Y_hatpst, 'atet': atet} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78ede04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_data_from_synth(df):    \n",
    "    df_pt = df[['unitid','T','Y']].pivot_table(columns='unitid', index='T', values='Y')\n",
    "    df_pt.reset_index(inplace=True)\n",
    "    treatment_time = df.loc[df['treatment']==1]['T'].min()\n",
    "    treated_units = df.loc[(df['treatment_units']==1)]['unitid'].unique()\n",
    "    control_pre = df_pt.loc[(df_pt['T']< treatment_time)][[e for e in df_pt.columns if e not in treated_units]]\n",
    "    control_pst = df_pt.loc[(df_pt['T']>=treatment_time)][[e for e in df_pt.columns if e not in treated_units]]\n",
    "\n",
    "    treat_pre = df_pt.loc[(df_pt['T']< treatment_time)][treated_units[0]]\n",
    "    treat_pst = df_pt.loc[(df_pt['T']>=treatment_time)][treated_units[0]]\n",
    "    return control_pre, control_pst, treat_pre, treat_pst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df6e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = synth_panel()\n",
    "control_pre, control_pst, treat_pre, treat_pst = sc_data_from_synth(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ebebd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.6327271025076378\n",
      "            Iterations: 12\n",
      "            Function evaluations: 916\n",
      "            Gradient evaluations: 12\n",
      "dict_keys(['weights', 'control_pre', 'control_pst', 'atet'])\n"
     ]
    }
   ],
   "source": [
    "## Estimate the treatment effect without any perturbation\n",
    "sc_output = syncontrol(X1=control_pst.drop(columns ='T'), X0=control_pre.drop(columns='T'),\n",
    "                      Y1=treat_pst, Y0=treat_pre)\n",
    "print(sc_output.keys())\n",
    "\n",
    "## Calculate the residual\n",
    "residual = np.concatenate( [sc_output['control_pre'] - treat_pre,\n",
    "                 sc_output['control_pst'] - treat_pst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we want to scramble the dates. Do this by replacing the date index.\n",
    "scrambled_dates = np.random.choice( len(df['T'].unique()),len(df['T'].unique()), replace=False )\n",
    "\n",
    "num_treated_time = len(df.loc[ df['treatment']==1]['T'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a68b60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrambled_sc(df, scrambled_dates, num_treated_time):\n",
    "    df_scrambled = df.copy()\n",
    "    df_scrambled['T'].replace(df_scrambled['T'].unique(), scrambled_dates, inplace=True)\n",
    "    df_scrambled['treatment'] = (df_scrambled['T']>=num_treated_time).astype(float)\n",
    "\n",
    "    ## Create the SC data\n",
    "    control_pre, control_pst, treat_pre, treat_pst = sc_data_from_synth(df_scrambled)\n",
    "\n",
    "    ## Train the SC model\n",
    "    sc_output = syncontrol(X1=control_pst.drop(columns ='T'), X0=control_pre.drop(columns='T'),\n",
    "                          Y1=treat_pst, Y0=treat_pre)\n",
    "\n",
    "    ## Calculate the residual\n",
    "    residual = np.concatenate( [sc_output['control_pre'] - treat_pre,\n",
    "                     sc_output['control_pst'] - treat_pst])\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33ccae5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e124676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
