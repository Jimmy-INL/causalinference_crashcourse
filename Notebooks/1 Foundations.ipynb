{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference Examples\n",
    "# 1 Foundations\n",
    "Julian Hsu\n",
    "Date Made: 5 Aug 2021 \n",
    "\n",
    "### Table of Contents with Navigation Links\n",
    "* [Write Causal Models](#Section1)\n",
    "* [Simulate Data](#Section2)\n",
    "* [Bootstrapping Examples](#Section3)\n",
    "* [Bootstrapping Examples - unconfoundedness violation](#Section4)\n",
    "* [Bootstrapping Examples - overlap violation](#Section5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os \n",
    "\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.discrete.conditional_models import ConditionalLogit\n",
    "\n",
    "from IPython.display import display    \n",
    "\n",
    "\n",
    "import scipy.stats \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, LassoCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section1'></a>\n",
    "\n",
    "## Write Causal Models\n",
    "Write several functions here for estimate HTE. Each model _must_ do datasplitting.\n",
    "These functions will do a lot of predictions, so try to standardize the prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stnomics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section2'></a>\n",
    "\n",
    "## Bring in Simulated Data\n",
    "Pretend we've never seen this data before, and do balance checks between treatment and control \n",
    "\n",
    "For fun, use the Friedman function: https://www.sfu.ca/~ssurjano/fried.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    N = 2000\n",
    "    \n",
    "    cov = [[1.00, 0.08, 0.05, 0.05],\n",
    "           [0.08, 1.00,-0.08,-0.02],\n",
    "           [0.05,-0.08, 1.00,-0.10],\n",
    "           [0.05,-0.02,-0.10, 1.00]]\n",
    "    cov = np.eye(4)\n",
    "    X = np.random.multivariate_normal(np.zeros(4), cov,N)\n",
    "    x1,x2,x3,x4= X[:,0],X[:,1],X[:,2],X[:,3]\n",
    "\n",
    "    treatment_latent = 2*np.sin( np.pi * x4 * x3) + 10*(x2-0.5)**2 - 10*x1\n",
    "    m,s = np.average(treatment_latent), np.std(treatment_latent)\n",
    "\n",
    "    treatment_latent = (treatment_latent - m) / s\n",
    "    \n",
    "    random_t = np.random.normal(0,1,N)\n",
    "    \n",
    "    treatment_latent += random_t\n",
    "    \n",
    "    treatment = np.array( np.exp(treatment_latent) / (1+ np.exp(treatment_latent)) > np.random.uniform(0,1,N) ).astype(np.int32)\n",
    "\n",
    "#     Y = 100 +0.5*x1 - 6*x2 + -2*x4*x1 + 0.5*x1*x2 - 7*(x3+1)**(0.5) + 8/(0.5+x3+x4)\n",
    "    Y = 100 + 10*np.sin( np.pi * x1 * x2) + 20*(x3-0.5)**2 - 10*x4\n",
    "#     GT = np.std(Y)\n",
    "    random_y = np.random.normal(0,1,N)\n",
    "\n",
    "    GT = 5\n",
    "    Y += np.random.normal(1,2,N)\n",
    "    Y += GT*(treatment==1) \n",
    "    \n",
    "    df_est = pd.DataFrame({'x1':x1, 'x2':x2,'x3':x3,'x4':x4,'treatment':treatment, 'Y':Y, 'GT':GT} )\n",
    "    df_est['x1_2'] = df_est['x1'].pow(2)\n",
    "    df_est['x2_2'] = df_est['x2'].pow(2)\n",
    "    df_est['x3_2'] = df_est['x3'].pow(2)\n",
    "    df_est['x4_2'] = df_est['x4'].pow(2)    \n",
    "    return df_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>treatment</th>\n",
       "      <th>Y</th>\n",
       "      <th>GT</th>\n",
       "      <th>x1_2</th>\n",
       "      <th>x2_2</th>\n",
       "      <th>x3_2</th>\n",
       "      <th>x4_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.313283</td>\n",
       "      <td>-0.489695</td>\n",
       "      <td>0.563628</td>\n",
       "      <td>-0.324982</td>\n",
       "      <td>1</td>\n",
       "      <td>105.700209</td>\n",
       "      <td>5</td>\n",
       "      <td>0.098146</td>\n",
       "      <td>0.239801</td>\n",
       "      <td>0.317676</td>\n",
       "      <td>0.105613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.347911</td>\n",
       "      <td>0.829529</td>\n",
       "      <td>0.863875</td>\n",
       "      <td>-0.082179</td>\n",
       "      <td>1</td>\n",
       "      <td>102.593495</td>\n",
       "      <td>5</td>\n",
       "      <td>0.121042</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>0.746279</td>\n",
       "      <td>0.006753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.674069</td>\n",
       "      <td>1.074357</td>\n",
       "      <td>0.852527</td>\n",
       "      <td>0.582977</td>\n",
       "      <td>0</td>\n",
       "      <td>105.036677</td>\n",
       "      <td>5</td>\n",
       "      <td>2.802507</td>\n",
       "      <td>1.154242</td>\n",
       "      <td>0.726802</td>\n",
       "      <td>0.339862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.288611</td>\n",
       "      <td>-0.335995</td>\n",
       "      <td>-1.054658</td>\n",
       "      <td>-0.091539</td>\n",
       "      <td>0</td>\n",
       "      <td>144.336512</td>\n",
       "      <td>5</td>\n",
       "      <td>0.083296</td>\n",
       "      <td>0.112893</td>\n",
       "      <td>1.112303</td>\n",
       "      <td>0.008379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.972706</td>\n",
       "      <td>0.119425</td>\n",
       "      <td>-1.156609</td>\n",
       "      <td>-0.030407</td>\n",
       "      <td>0</td>\n",
       "      <td>152.401670</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946158</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>1.337744</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.644172</td>\n",
       "      <td>-0.381519</td>\n",
       "      <td>2.104919</td>\n",
       "      <td>-0.124986</td>\n",
       "      <td>0</td>\n",
       "      <td>158.054612</td>\n",
       "      <td>5</td>\n",
       "      <td>0.414957</td>\n",
       "      <td>0.145557</td>\n",
       "      <td>4.430683</td>\n",
       "      <td>0.015621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.018871</td>\n",
       "      <td>-1.890406</td>\n",
       "      <td>0.717081</td>\n",
       "      <td>1.377236</td>\n",
       "      <td>1</td>\n",
       "      <td>96.820982</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>3.573634</td>\n",
       "      <td>0.514205</td>\n",
       "      <td>1.896778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.782518</td>\n",
       "      <td>-0.175322</td>\n",
       "      <td>-0.079507</td>\n",
       "      <td>0.643935</td>\n",
       "      <td>1</td>\n",
       "      <td>97.958290</td>\n",
       "      <td>5</td>\n",
       "      <td>0.612335</td>\n",
       "      <td>0.030738</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.414652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-1.698061</td>\n",
       "      <td>-0.853848</td>\n",
       "      <td>0.383134</td>\n",
       "      <td>1.211971</td>\n",
       "      <td>1</td>\n",
       "      <td>81.896919</td>\n",
       "      <td>5</td>\n",
       "      <td>2.883411</td>\n",
       "      <td>0.729056</td>\n",
       "      <td>0.146792</td>\n",
       "      <td>1.468875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-1.783112</td>\n",
       "      <td>0.654464</td>\n",
       "      <td>0.445458</td>\n",
       "      <td>-0.876325</td>\n",
       "      <td>1</td>\n",
       "      <td>119.786695</td>\n",
       "      <td>5</td>\n",
       "      <td>3.179490</td>\n",
       "      <td>0.428324</td>\n",
       "      <td>0.198432</td>\n",
       "      <td>0.767946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4  treatment           Y  GT  \\\n",
       "0     0.313283 -0.489695  0.563628 -0.324982          1  105.700209   5   \n",
       "1    -0.347911  0.829529  0.863875 -0.082179          1  102.593495   5   \n",
       "2    -1.674069  1.074357  0.852527  0.582977          0  105.036677   5   \n",
       "3     0.288611 -0.335995 -1.054658 -0.091539          0  144.336512   5   \n",
       "4    -0.972706  0.119425 -1.156609 -0.030407          0  152.401670   5   \n",
       "...        ...       ...       ...       ...        ...         ...  ..   \n",
       "1995 -0.644172 -0.381519  2.104919 -0.124986          0  158.054612   5   \n",
       "1996 -0.018871 -1.890406  0.717081  1.377236          1   96.820982   5   \n",
       "1997  0.782518 -0.175322 -0.079507  0.643935          1   97.958290   5   \n",
       "1998 -1.698061 -0.853848  0.383134  1.211971          1   81.896919   5   \n",
       "1999 -1.783112  0.654464  0.445458 -0.876325          1  119.786695   5   \n",
       "\n",
       "          x1_2      x2_2      x3_2      x4_2  \n",
       "0     0.098146  0.239801  0.317676  0.105613  \n",
       "1     0.121042  0.688119  0.746279  0.006753  \n",
       "2     2.802507  1.154242  0.726802  0.339862  \n",
       "3     0.083296  0.112893  1.112303  0.008379  \n",
       "4     0.946158  0.014262  1.337744  0.000925  \n",
       "...        ...       ...       ...       ...  \n",
       "1995  0.414957  0.145557  4.430683  0.015621  \n",
       "1996  0.000356  3.573634  0.514205  1.896778  \n",
       "1997  0.612335  0.030738  0.006321  0.414652  \n",
       "1998  2.883411  0.729056  0.146792  1.468875  \n",
       "1999  3.179490  0.428324  0.198432  0.767946  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_iter = 500\n",
    "## treatment prediction models\n",
    "t_models = {}\n",
    "t_models['LogitCV'] = LogisticRegressionCV(cv=5, random_state=27, n_jobs=-1)\n",
    "t_models['logit'] = LogisticRegression(penalty='l2',solver='lbfgs', C=1, max_iter=model_max_iter, fit_intercept=True)\n",
    "t_models['logit_L1_C2'] = LogisticRegression(penalty='l1',C=2, max_iter=model_max_iter, fit_intercept=True)\n",
    "t_models['logit_L2_C5'] = LogisticRegression(penalty='l2',C=2, max_iter=model_max_iter, fit_intercept=True)\n",
    "t_models['rf_md10'] = RandomForestClassifier(n_estimators=25,max_depth=10, min_samples_split=200,n_jobs=-1)\n",
    "t_models['rf_md3'] = RandomForestClassifier(n_estimators=25,max_depth=3, min_samples_split=200,n_jobs=-1)\n",
    "t_models['nn'] = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1,max_iter=model_max_iter)\n",
    "## outcome prediction models\n",
    "y_models = {}\n",
    "y_models['LassoCV'] = LassoCV(cv=5, n_jobs=-1,  random_state=27)\n",
    "y_models['ols'] = LinearRegression()\n",
    "y_models['lasso_a2'] = Lasso(alpha=2,max_iter=model_max_iter)\n",
    "y_models['ridge_a2'] = Ridge(alpha=2,max_iter=model_max_iter)\n",
    "y_models['rf_md10'] = RandomForestRegressor(n_estimators=25,max_depth=10, min_samples_split=200,n_jobs=-1)\n",
    "y_models['rf_md3'] = RandomForestRegressor(n_estimators=25,max_depth=3, min_samples_split=200,n_jobs=-1)\n",
    "y_models['nn'] = MLPRegressor(alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1, max_iter=model_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_splits = 4\n",
    "aux_dictionary = {'n_bins': 2, 'n_trees':2, 'max_depth':2, \n",
    "                  'upper':0.999, 'lower':0.001,\n",
    "                  'bootstrapreps':100,\n",
    "                  'subsample_ratio':0.5}\n",
    "bootstrap_number = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_data()\n",
    "\n",
    "feature_list = [x for x in df.columns if 'x' in x]\n",
    "\n",
    "ols = st.ate.ols_vanilla(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "pbin = st.ate.propbinning(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "plm = st.ate.dml.dml_plm(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "irm = st.ate.dml.dml_irm(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "ip = st.ate.ipw(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_data()\n",
    "df['splits'] = np.random.choice(n_data_splits, len(df), replace=True)\n",
    "df = df.sort_values(by='splits')    \n",
    "\n",
    "## Predict Treatment\n",
    "that = st.predict_treatment_indicator(df, 'splits', n_data_splits, feature_list,'treatment',t_models['LogitCV'])\n",
    "df['that'] = that\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1, figsize=(9,3), sharex=True, sharey=True)\n",
    "ax.hist(df.loc[df.treatment==1]['that'], density=False, facecolor='g', alpha=0.25)\n",
    "ax.hist(df.loc[df.treatment==0]['that'], density=False, facecolor='b', alpha=0.25)\n",
    "control_range_to_remove = np.percentile(df.loc[df.treatment==1]['that'], q= 50) , np.percentile(df.loc[df.treatment==1]['that'], q= 99)\n",
    "print(control_range_to_remove)\n",
    "\n",
    "df = df.loc[ (df.treatment==1) | ( (df.that.between(control_range_to_remove[0],control_range_to_remove[1])==False) & (df.treatment==0) )   ]\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1, figsize=(9,3), sharex=True, sharey=True)\n",
    "ax.hist(df.loc[df.treatment==1]['that'], density=False, facecolor='g', alpha=0.25)\n",
    "ax.hist(df.loc[df.treatment==0]['that'], density=False, facecolor='b', alpha=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section4'></a>\n",
    "\n",
    "## Bootstrapping\n",
    "* Bootstrap results using random datasets when all three assumptions are satisfied.\n",
    "* Bootstrap results when the unconfoundedness assumption is violated. Do this by removing one fot the features from training.\n",
    "* Bootstrap results when the overlap assumption is violated. Do this by removing control observations with propensities near the median treatment obervation propensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_x = []\n",
    "pbin_x= []\n",
    "plm_x = []\n",
    "irm_x = []\n",
    "ipw_x = []\n",
    "\n",
    "ols_x_unconf = []\n",
    "pbin_x_unconf= []\n",
    "plm_x_unconf = []\n",
    "irm_x_unconf = []\n",
    "ipw_x_unconf = []\n",
    "\n",
    "ols_x_overlap = []\n",
    "pbin_x_overlap= []\n",
    "plm_x_overlap = []\n",
    "irm_x_overlap = []\n",
    "ipw_x_overlap = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(bootstrap_number):\n",
    "    df = generate_data()\n",
    "    \n",
    "    feature_list = [x for x in df.columns if 'x' in x]\n",
    "    \n",
    "    feature_list_ab = [x for x in feature_list if '3' not in x and '4' not in x]\n",
    "    \n",
    "    ## Regular \n",
    "    ols = st.ate.ols_vanilla(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    pbin = st.ate.propbinning(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    plm = st.ate.dml.dml_plm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    irm = st.ate.dml.dml_irm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    ip = st.ate.ipw(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "    ols_x.append(ols['ATE TE'])\n",
    "    pbin_x.append(pbin['ATE TE'])\n",
    "    plm_x.append(plm['ATE TE'])\n",
    "    irm_x.append(irm['ATE TE'])    \n",
    "    ipw_x.append(ip['ATE TE'])   \n",
    "    \n",
    "    ## When unconfoundedness assumption is not true\n",
    "    ols = st.ate.ols_vanilla(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    pbin = st.ate.propbinning(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    plm = st.ate.dml.dml_plm(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    irm = st.ate.dml.dml_irm(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    ip = st.ate.ipw(df, \n",
    "                'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "    ols_x_unconf.append(ols['ATE TE'])\n",
    "    pbin_x_unconf.append(pbin['ATE TE'])\n",
    "    plm_x_unconf.append(plm['ATE TE'])\n",
    "    irm_x_unconf.append(irm['ATE TE'])    \n",
    "    ipw_x_unconf.append(ip['ATE TE'])        \n",
    "\n",
    "\n",
    "    ## When overlap condition is not true\n",
    "    df['splits'] = np.random.choice(n_data_splits, len(df), replace=True)\n",
    "    df = df.sort_values(by='splits')    \n",
    "    ## Predict Treatment\n",
    "    that = st.predict_treatment_indicator(df, 'splits', n_data_splits, feature_list,'treatment',t_models['LogitCV'])\n",
    "    df['that'] = that    \n",
    "    control_range_to_remove = np.percentile(df.loc[df.treatment==1]['that'], q= 50) , np.percentile(df.loc[df.treatment==1]['that'], q= 99)\n",
    "    df = df.loc[ (df.treatment==1) | ( (df.that.between(control_range_to_remove[0],control_range_to_remove[1])==False) & (df.treatment==0) )   ]\n",
    "\n",
    "\n",
    "    ols = st.ate.ols_vanilla(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    pbin = st.ate.propbinning(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    plm = st.ate.dml.dml_plm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    irm = st.ate.dml.dml_irm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    ip = st.ate.ipw(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "\n",
    "    ols_x_overlap.append(ols['ATE TE'])\n",
    "    pbin_x_overlap.append(pbin['ATE TE'])\n",
    "    plm_x_overlap.append(plm['ATE TE'])\n",
    "    irm_x_overlap.append(irm['ATE TE'])    \n",
    "    ipw_x_overlap.append(ip['ATE TE'])        \n",
    "\n",
    "ols_x = np.array(ols_x) - 5\n",
    "pbin_x = np.array(pbin_x) - 5\n",
    "plm_x = np.array(plm_x) - 5\n",
    "irm_x = np.array(irm_x) - 5\n",
    "ipw_x = np.array(ipw_x) - 5\n",
    "\n",
    "ols_x_unconf = np.array(ols_x_unconf) - 5\n",
    "pbin_x_unconf = np.array(pbin_x_unconf) - 5\n",
    "plm_x_unconf = np.array(plm_x_unconf) - 5\n",
    "irm_x_unconf = np.array(irm_x_unconf) - 5\n",
    "ipw_x_unconf = np.array(ipw_x_unconf) - 5    \n",
    "\n",
    "ols_x_overlap = np.array(ols_x_overlap) - 5\n",
    "pbin_x_overlap = np.array(pbin_x_overlap) - 5\n",
    "plm_x_overlap = np.array(plm_x_overlap) - 5\n",
    "irm_x_overlap = np.array(irm_x_overlap) - 5\n",
    "ipw_x_overlap = np.array(ipw_x_overlap) - 5    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_avg_med_iqr(x):\n",
    "    avg = np.average(x)\n",
    "    p50 = np.percentile(x, 50)\n",
    "    p25 = np.percentile(x, 25)\n",
    "    p75 = np.percentile(x, 75)    \n",
    "    print('AVG: {0:5.2f}   MED: {1:5.2f}   IQR: [{2:5.3f}, {3:5.2f}]'.format(avg, p50, p25, p75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bias when all assumptions are met')\n",
    "print_avg_med_iqr(ols_x)    \n",
    "print_avg_med_iqr(pbin_x)    \n",
    "print_avg_med_iqr(plm_x)    \n",
    "print_avg_med_iqr(irm_x)    \n",
    "print_avg_med_iqr(ipw_x)    \n",
    "\n",
    "print('')\n",
    "print('Bias when unconfoundedness is not met')\n",
    "print_avg_med_iqr(ols_x_unconf) \n",
    "print_avg_med_iqr(pbin_x_unconf)    \n",
    "print_avg_med_iqr(plm_x_unconf)    \n",
    "print_avg_med_iqr(irm_x_unconf)    \n",
    "print_avg_med_iqr(ipw_x_unconf)    \n",
    "\n",
    "print('')\n",
    "print('Bias when overlap is not met')\n",
    "print_avg_med_iqr(ols_x_overlap)    \n",
    "print_avg_med_iqr(pbin_x_overlap)    \n",
    "print_avg_med_iqr(plm_x_overlap)    \n",
    "print_avg_med_iqr(irm_x_overlap)    \n",
    "print_avg_med_iqr(ipw_x_overlap[~np.isnan(ipw_x_overlap)])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section5'></a>\n",
    "\n",
    "## Prediction vs Causal\n",
    "Let's compare the estimated treatment effects $\\hat{Y}(W=1) - \\hat{Y}(W=0) $ among ML models. Let's use the treatment effect of multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data(WDim=2,\n",
    "             TE = [1,1],\n",
    "             N = 50):\n",
    "    corr = False\n",
    "    if corr==False:\n",
    "        pass\n",
    "    else:\n",
    "        x = np.random.uniform(0,1,N)\n",
    "        \n",
    "    for r in range(WDim+1):\n",
    "        if corr==False:\n",
    "            W = np.random.randint(0,2, N)     \n",
    "        else: \n",
    "            x1 = np.random.uniform(-1,1,N)                \n",
    "            W = ( ( np.exp(x + x1) / (1+ np.exp(x+x1)) ) > np.random.uniform(0.45,0.55) ).astype(float)\n",
    "        if r ==0:\n",
    "            Y = TE[r]*W + np.random.normal(0,1, N)\n",
    "            data_dict = {'W1':W}\n",
    "        else:\n",
    "            Y = TE[r]*W\n",
    "            data_dict['W'+str(r)] = W\n",
    "    data_dict['Y'] = Y\n",
    "    return pd.DataFrame(data=data_dict, index=np.arange(N))\n",
    "#     if corr==False:\n",
    "#         W1 = np.random.randint(0,2, N) \n",
    "#         W2 = np.random.randint(0,2, N)     \n",
    "#     else:\n",
    "#         x = np.random.uniform(0,1,N)\n",
    "#         x1 = np.random.uniform(-1,1,N)\n",
    "#         x2 = np.random.uniform(-1,1,N)        \n",
    "#         W1 = ( ( np.exp(x + x1) / (1+ np.exp(x+x1)) ) > np.random.uniform(0.45,0.55) ).astype(float)\n",
    "#         W2 = ( ( np.exp(x + x2) / (1+ np.exp(x+x2)) ) > np.random.uniform(0.45,0.55) ).astype(float)        \n",
    "\n",
    "\n",
    "#     Y = TE[0]*W1 + TE[1]*W2 + np.random.normal(0,1, N)\n",
    "#     return pd.DataFrame(data={'Y':Y, 'W1':W1, 'W2': W2}, index=np.arange(N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_te(X, WDim, func):\n",
    "    trained = func.fit(X[[x for x in X.columns if 'W' in x]], X['Y'])\n",
    "    te_output = {}\n",
    "    for r in range(WDim+1):\n",
    "        T = np.zeros(WDim)\n",
    "        T[r-1] = 1\n",
    "        te_output[str(r)] = trained.predict([T])[0] - trained.predict([np.zeros(WDim)])[0]\n",
    "#     te1 = trained.predict([[1,0]]) - trained.predict([[0,0]])\n",
    "#     te2 = trained.predict([[0,1]]) - trained.predict([[0,0]])    \n",
    "    return te_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2 = MLPRegressor(hidden_layer_sizes=(2,), max_iter = 2000, random_state=4227)\n",
    "nn10 = MLPRegressor(hidden_layer_sizes=(10,), max_iter = 2000, random_state=4227)\n",
    "ols = LinearRegression()\n",
    "rf1000 = RandomForestRegressor(n_estimators=1000)\n",
    "rf100 = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WDim_use = 3\n",
    "TE_use = [0.05]*(WDim_use+1)\n",
    "df = sim_data(WDim=WDim_use, TE = TE_use, N = 100)\n",
    "ols_AH = ml_te(df, WDim_use, ols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Data and simulate many OLS and other estimates\n",
    "dict_est = {}\n",
    "for w in range(WDim_use):\n",
    "    dict_est[str(w)] = {'OLS':[], 'NN2':[], 'NN10':[], 'RF1000':[], 'RF100':[]  }\n",
    "\n",
    "for r in range(5):\n",
    "    df = sim_data(WDim=WDim_use, TE = TE_use, N = 100)\n",
    "    # display(df.describe())\n",
    "    ols_HAT = ml_te(df,   WDim_use, ols)\n",
    "    nn2_HAT = ml_te(df,   WDim_use, nn2)\n",
    "    nn10_HAT = ml_te(df,  WDim_use, nn10)\n",
    "    rf1000_HAT = ml_te(df,WDim_use, rf1000)\n",
    "    rf100_HAT = ml_te(df, WDim_use, rf100)\n",
    "    for w in range(WDim_use):\n",
    "        dict_est[str(w)]['OLS'] = ols_HAT[str(w)]\n",
    "        dict_est[str(w)]['NN2'] = nn2_HAT[str(w)]\n",
    "        dict_est[str(w)]['NN10'] = nn10_HAT[str(w)]\n",
    "        dict_est[str(w)]['RF1000'] = rf1000_HAT[str(w)]\n",
    "        dict_est[str(w)]['RF100'] = rf100_HAT[str(w)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Parameter 0\n",
      "OLS\n",
      " |Bias| of Est = 1.39e-17 using OLS\n",
      "NN2\n",
      " |Bias| of Est = 0.108 using NN2\n",
      "NN10\n",
      " |Bias| of Est = 0.112 using NN10\n",
      "RF1000\n",
      " |Bias| of Est = 7.08e-16 using RF1000\n",
      "RF100\n",
      " |Bias| of Est = 9.02e-17 using RF100\n",
      "For Parameter 1\n",
      "OLS\n",
      " |Bias| of Est =  0.05 using OLS\n",
      "NN2\n",
      " |Bias| of Est =  0.12 using NN2\n",
      "NN10\n",
      " |Bias| of Est = 0.402 using NN10\n",
      "RF1000\n",
      " |Bias| of Est =  0.05 using RF1000\n",
      "RF100\n",
      " |Bias| of Est =  0.05 using RF100\n",
      "For Parameter 2\n",
      "OLS\n",
      " |Bias| of Est =  0.05 using OLS\n",
      "NN2\n",
      " |Bias| of Est = 0.232 using NN2\n",
      "NN10\n",
      " |Bias| of Est = 0.342 using NN10\n",
      "RF1000\n",
      " |Bias| of Est =  0.05 using RF1000\n",
      "RF100\n",
      " |Bias| of Est =  0.05 using RF100\n"
     ]
    }
   ],
   "source": [
    "for w in range(WDim_use):\n",
    "    print('For Parameter {0}'.format(w))\n",
    "    for a in ['OLS','NN2','NN10','RF1000','RF100']:\n",
    "        print(a)\n",
    "        est1_bias = np.abs( np.mean(dict_est[str(w)][a] ) - TE_use[w] )\n",
    "        print(' |Bias| of Est = {0:5.3} using {1}'.format(est1_bias, a))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Estimate 2')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(nrows=2,ncols=2, figsize=(15,8), sharex=True, sharey=False)\n",
    "ax[0,0].hist(dict_est1['OLS'],   density=False,  label='ols',           alpha=0.25, color='coral')\n",
    "ax[0,0].hist(dict_est1['NN2'],   density=False,  label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "# ax[0,0].hist(dict_est1['NN10'],  density=False,  label='neural net 10', alpha=0.25, color='lime')\n",
    "# ax[0,0].hist(dict_est1['RF1000'],density=False,  label='RF 1000',       alpha=0.25,  color='navy')\n",
    "ax[0,0].hist(dict_est1['RF100'], density=False,  label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "ax[0,0].vlines(TE_use[0], 0, 50, colors='black', label='Truth')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].grid()\n",
    "ax[0,0].set_title('Estimate 1')\n",
    "\n",
    "ax[1,0].hist(dict_est1['OLS'],   density=False,  label='ols',           alpha=0.25, color='coral')\n",
    "# ax[1,0].hist(dict_est1['NN2'],   density=False,  label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "ax[1,0].hist(dict_est1['NN10'],  density=False,  label='neural net 10', alpha=0.25, color='lime')\n",
    "ax[1,0].hist(dict_est1['RF1000'],density=False,  label='RF 1000',       alpha=0.25,  color='navy')\n",
    "# ax[1,0].hist(dict_est1['RF100'], density=False,  label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "ax[1,0].vlines(TE_use[0], 0, 50, colors='black', label='Truth')\n",
    "ax[1,0].legend()\n",
    "ax[1,0].grid()\n",
    "ax[1,0].set_title('Estimate 1')\n",
    "\n",
    "ax[0,1].hist(dict_est2['OLS'],   density=False, label='ols',           alpha=0.25, color='coral')\n",
    "ax[0,1].hist(dict_est2['NN2'],   density=False, label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "# ax[0,1].hist(dict_est2['NN10'],  density=False, label='neural net 10', alpha=0.25, color='lime')\n",
    "# ax[0,1].hist(dict_est2['RF1000'],density=False, label='RF 1000',       alpha=0.25,  color='navy')\n",
    "ax[0,1].hist(dict_est2['RF100'], density=False, label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "ax[0,1].vlines(TE_use[1], 0, 50, colors='black', label='Truth')\n",
    "ax[0,1].legend()\n",
    "ax[0,1].grid()\n",
    "ax[0,1].set_title('Estimate 2')\n",
    "\n",
    "ax[1,1].hist(dict_est2['OLS'],   density=False, label='ols',           alpha=0.25, color='coral')\n",
    "# ax[1,1].hist(dict_est2['NN2'],   density=False, label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "ax[1,1].hist(dict_est2['NN10'],  density=False, label='neural net 10', alpha=0.25, color='lime')\n",
    "ax[1,1].hist(dict_est2['RF1000'],density=False, label='RF 1000',       alpha=0.25,  color='navy')\n",
    "# ax[1,1].hist(dict_est2['RF100'], density=False, label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "ax[1,1].vlines(TE_use[1], 0, 50, colors='black', label='Truth')\n",
    "ax[1,1].legend()\n",
    "ax[1,1].grid()\n",
    "ax[1,1].set_title('Estimate 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
